#!/usr/bin/env python3
"""
Enhanced FinTalk.AI - ä¸»å…¥å£
æ•´åˆæ‰€æœ‰MCPæ ¸å¿ƒåŠŸèƒ½æ¨¡å—
"""

import os
import sys
import json
import time
import logging
from typing import Dict, Any, Optional

# Setup path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import core modules
from enhanced_core import (
    ParallelExecutor,
    QueryRewriter,
    QueryArbitrator,
    RejectionDetector,
    CorrelationChecker,
    FinancialFunctionRegistry,
    StreamingNLG,
    ConversationManager
)

# Import existing components
from formula import find_formula_for_query, calculate_from_expression
from OSWorld.docker_osworld_adapter import DockerOSWorldAdapter

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============== Configuration ==============
API_URL = "https://qianfan.baidubce.com/v2/chat/completions"
API_KEY = "bce-v3/ALTAK-dgZMQj7E5tByoRofFKlbM/e852481aaab5ebf3ffe6f2a50589e6e41646c127"


# ============== LLM Caller ==============
def llm_caller(prompt: str, temperature: float = 0.3) -> str:
    """LLMè°ƒç”¨å‡½æ•°"""
    import requests
    payload = {
        "model": "deepseek-v3.2-think",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "web_search": {"enable": False}
    }
    try:
        response = requests.post(API_URL, headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}"
        }, json=payload, timeout=30)
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        logger.error(f"LLM API error: {e}")
        return ""


# ============== Enhanced FinTalk.AI ==============
class EnhancedFinTalkAI:
    """
    å¢å¼ºç‰ˆFinTalk.AI

    é›†æˆæ‰€æœ‰MCPæ ¸å¿ƒåŠŸèƒ½ï¼š
    1. âœ… å¹¶è¡Œæ¨¡å‹è°ƒç”¨
    2. âœ… Queryæ”¹å†™
    3. âœ… ä»²è£æœºåˆ¶
    4. âœ… æ‹’è¯†æ£€æµ‹
    5. âœ… ç›¸å…³æ€§åˆ¤æ–­
    6. âœ… Function Calling
    7. âœ… æµå¼è¾“å‡º
    8. âœ… NLU/NLG
    9. âœ… å¯¹è¯ç®¡ç†
    """

    def __init__(self, use_osworld: bool = True):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆFinTalk.AI"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ Enhanced FinTalk.AI - MCP Architecture")
        logger.info("="*80)

        # åˆå§‹åŒ–OSWorldæˆ–æœ¬åœ°æ•°æ®åº“
        if use_osworld:
            self.adapter = DockerOSWorldAdapter()
            self.db = None
            self.env_mode = "Docker OSWorld"
        else:
            self.adapter = None
            self._init_local_db()
            self.env_mode = "Local SQLite"

        # åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒæ¨¡å—
        self.parallel_executor = ParallelExecutor(max_workers=10)
        self.query_rewriter = QueryRewriter(llm_caller)
        self.arbitrator = QueryArbitrator(llm_caller)
        self.rejection_detector = RejectionDetector(llm_caller)
        self.correlation_checker = CorrelationChecker(llm_caller)
        self.function_registry = FinancialFunctionRegistry(self.db, self.adapter)
        self.nlg = StreamingNLG(API_URL, API_KEY)
        self.conversation_manager = ConversationManager()

        logger.info(f"âœ… Environment: {self.env_mode}")
        logger.info(f"âœ… All modules initialized")

    def _init_local_db(self):
        """åˆå§‹åŒ–æœ¬åœ°æ•°æ®åº“"""
        import pandas as pd
        import sqlite3

        self.db = sqlite3.connect(':memory:', check_same_thread=False)
        data_dir = "data"

        csv_files = {
            "companies": os.path.join(data_dir, "company.csv"),
            "management": os.path.join(data_dir, "management.csv"),
            "shareholders": os.path.join(data_dir, "shareholder.csv")
        }

        for table_name, file_path in csv_files.items():
            if os.path.exists(file_path):
                try:
                    df = pd.read_csv(file_path, encoding='utf-8', encoding_errors='ignore')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='latin-1')
                df.to_sql(table_name, self.db, if_exists='replace', index=False)
                logger.info(f"   Loaded {len(df)} rows into '{table_name}'")

    def process_query(self,
                     user_query: str,
                     sender_id: str = "test",
                     stream_output: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·queryï¼ˆä½¿ç”¨æ‰€æœ‰MCPæ ¸å¿ƒåŠŸèƒ½ï¼‰

        Args:
            user_query: ç”¨æˆ·query
            sender_id: ç”¨æˆ·ID
            stream_output: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()

        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ‘¤ User Query: {user_query}")
        logger.info(f"ğŸ’¬ Context: {self.conversation_manager.get_context_summary()}")
        logger.info(f"{'='*80}")

        # è·å–å¯¹è¯å†å²
        history_text = self.conversation_manager.get_history_text(n_turns=3)
        prev_query = self.conversation_manager.get_last_query()

        # ============== STEP 1: å¹¶è¡Œæ¨¡å‹è°ƒç”¨ ==============
        logger.info(f"\nğŸ“ STEP 1: Parallel Model Calls")

        def task_rewrite():
            return self.query_rewriter.rewrite(user_query, history_text)

        def task_arbitrate():
            return self.arbitrator.arbitrate(user_query, history_text)

        def task_rejection():
            return self.rejection_detector.should_accept(user_query)

        def task_correlation():
            return self.correlation_checker.is_correlated(
                prev_query or "", user_query
            )

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        tasks = {
            "rewrite": task_rewrite,
            "arbitrate": task_arbitrate,
            "rejection": task_rejection,
            "correlation": task_correlation
        }

        parallel_results = self.parallel_executor.execute_parallel(tasks, timeout=60)

        # æå–ç»“æœ
        rewritten_query = parallel_results["rewrite"].result
        arbitration = parallel_results["arbitrate"].result
        accept = parallel_results["rejection"].result
        is_correlated = parallel_results["correlation"].result

        logger.info(f"\nğŸ“Š Parallel Results:")
        logger.info(f"   Rewrite: {user_query} -> {rewritten_query}")
        logger.info(f"   Type: {arbitration.query_type}")
        logger.info(f"   Accept: {accept}")
        logger.info(f"   Correlated: {is_correlated}")

        # ============== STEP 2: å¤„ç†æ‹’è¯† ==============
        if not accept:
            result = {
                "query": user_query,
                "rewritten_query": rewritten_query,
                "status": "rejected",
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åªèƒ½å›ç­”ä¸é‡‘èæ•°æ®åˆ†æç›¸å…³çš„é—®é¢˜ã€‚",
                "execution_time": time.time() - start_time
            }
            logger.info(f"âŒ Query rejected")
            return result

        # ============== STEP 3: æ ¹æ®ä»²è£ç»“æœå¤„ç† ==============
        logger.info(f"\nğŸ“ STEP 2: Execute by Type ({arbitration.query_type})")

        if arbitration.query_type == "task":
            # ä»»åŠ¡å¯¼å‘ - ä½¿ç”¨Function Calling
            answer = self._handle_task_query(rewritten_query, stream_output)

        elif arbitration.query_type == "knowledge":
            # çŸ¥è¯†æŸ¥è¯¢
            answer = self._handle_knowledge_query(rewritten_query)

        elif arbitration.query_type == "small_talk":
            # é—²èŠ
            answer = self._handle_small_talk(rewritten_query)

        else:
            # æ— æ•ˆè¾“å…¥
            answer = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ã€‚"

        # ============== STEP 3: æ›´æ–°å¯¹è¯å†å² ==============
        self.conversation_manager.add_turn(
            user_query,
            answer,
            arbitration.query_type
        )

        result = {
            "query": user_query,
            "rewritten_query": rewritten_query,
            "status": "success",
            "query_type": arbitration.query_type,
            "answer": answer,
            "execution_time": time.time() - start_time
        }

        logger.info(f"\nâœ… Completed in {result['execution_time']:.2f}s")
        logger.info(f"ğŸ’¬ Answer: {answer[:100]}...")

        return result

    def _handle_task_query(self, query: str, stream_output: bool) -> str:
        """å¤„ç†ä»»åŠ¡æŸ¥è¯¢"""
        logger.info(f"   ğŸ¯ Handling as task query")

        # æå–function call
        func_call = self._extract_function_call(query)

        if func_call:
            # æ‰§è¡Œfunction
            func_result = self.function_registry.execute(
                func_call["function_name"],
                func_call.get("parameters", {})
            )

            if "error" not in func_result:
                # ä½¿ç”¨NLGç”Ÿæˆç­”æ¡ˆ
                answer = self.nlg.generate_answer(query, func_result)
                return answer
            else:
                return f"æŠ±æ­‰ï¼Œ{func_result['error']}"

        # å°è¯•é€šç”¨å¤„ç†
        return self._handle_general_query(query)

    def _handle_knowledge_query(self, query: str) -> str:
        """å¤„ç†çŸ¥è¯†æŸ¥è¯¢"""
        logger.info(f"   ğŸ“š Handling as knowledge query")

        return llm_caller(
            f"ç”¨ç®€å•çš„è¯è§£é‡Šè¿™ä¸ªé‡‘èæ¦‚å¿µï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š{query}",
            temperature=0.7
        )

    def _handle_small_talk(self, query: str) -> str:
        """å¤„ç†é—²èŠ"""
        logger.info(f"   ğŸ’¬ Handling as small talk")

        responses = {
            "hello": "ä½ å¥½ï¼æˆ‘æ˜¯FinTalk.AIï¼Œä½ çš„é‡‘èæ•°æ®åˆ†æåŠ©æ‰‹ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
            "hi": "ä½ å¥½ï¼æˆ‘æ˜¯FinTalk.AIï¼Œè¯·é—®æœ‰ä»€ä¹ˆé‡‘èæ•°æ®ç›¸å…³çš„é—®é¢˜éœ€è¦æŸ¥è¯¢ï¼Ÿ",
            "thank": "ä¸å®¢æ°”ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ã€‚",
            "bye": "å†è§ï¼ç¥ä½ ä¸€åˆ‡é¡ºåˆ©ï¼"
        }

        query_lower = query.lower()
        for key, response in responses.items():
            if key in query_lower:
                return response

        return llm_caller(f"ç®€è¦å‹å¥½åœ°å›å¤ï¼š{query}", temperature=0.7)

    def _extract_function_call(self, query: str) -> Optional[Dict]:
        """æå–function call"""
        functions_json = json.dumps(self.function_registry.get_functions(), ensure_ascii=False)

        prompt = f"""You are a financial function calling expert. Extract the function call from this query.

Available functions:
{functions_json}

Query: {query}

Return JSON format:
{{
    "function_name": "function_name",
    "parameters": {{"key": "value"}}
}}

If no function matches, return {{"function_name": "none"}}"""

        result = llm_caller(prompt, temperature=0.1)

        try:
            result = result.replace("```json", "").replace("```", "").strip()
            if "{" in result and "}" in result:
                start = result.index("{")
                end = result.rindex("}") + 1
                func_call = json.loads(result[start:end])

                if func_call.get("function_name") != "none":
                    logger.info(f"   ğŸ”§ Function: {func_call['function_name']}")
                    return func_call

        except Exception as e:
            logger.warning(f"Function extraction failed: {e}")

        return None

    def _handle_general_query(self, query: str) -> str:
        """å¤„ç†é€šç”¨æŸ¥è¯¢"""
        # ç®€å•å¤„ç†ï¼šå¦‚æœæœ‰å…¬å¸åï¼Œè¿”å›å…¬å¸ä¿¡æ¯
        if "za bank" in query.lower():
            func_result = self.function_registry.execute("get_company_info", {"company_name": "ZA Bank"})
        elif "welab" in query.lower():
            func_result = self.function_registry.execute("get_company_info", {"company_name": "WeLab Bank"})
        else:
            return "è¯·æŒ‡å®šä½ æƒ³æŸ¥è¯¢çš„å…¬å¸åç§°ï¼ˆå¦‚ï¼šZA Bank æˆ– WeLab Bankï¼‰"

        if "error" not in func_result:
            return self.nlg.generate_answer(query, func_result)
        else:
            return func_result["error"]

    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.adapter:
            self.adapter.close()
        elif hasattr(self, 'db') and self.db:
            self.db.close()
        logger.info("âœ… Resources cleaned up")


# ============== Demo ==============
def demo_enhanced():
    """æ¼”ç¤ºå¢å¼ºç‰ˆFinTalk.AI"""

    print("\n" + "="*80)
    print("ğŸš€ Enhanced FinTalk.AI - MCP Core Features")
    print("   å¹¶è¡Œè°ƒç”¨ | Queryæ”¹å†™ | ä»²è£ | æ‹’è¯† | Function Calling | æµå¼è¾“å‡º | å¯¹è¯ç®¡ç†")
    print("="*80)

    # åˆå§‹åŒ–
    client = EnhancedFinTalkAI(use_osworld=True)

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "Hello!",
        "What is ZA Bank's employee size?",
        "And WeLab?",
        "Calculate executive_director_ratio for ZA Bank",
        "How is executive_director_ratio calculated?",
        "Compare ZA Bank and WeLab Bank on executive_director_ratio"
    ]

    for i, query in enumerate(test_cases, 1):
        print(f"\n{'â”€'*80}")
        print(f"Test {i}/{len(test_cases)}: {query}")
        print(f"{'â”€'*80}")

        result = client.process_query(query, stream_output=False)

        print(f"\nStatus: {result['status']}")
        print(f"Answer: {result['answer']}")
        print(f"Time: {result['execution_time']:.2f}s")

    # æ˜¾ç¤ºå¯¹è¯å†å²
    print(f"\n{'='*80}")
    print("ğŸ’¬ Conversation History:")
    print(f"{'='*80}")
    for turn in client.conversation_manager.history:
        print(f"User: {turn.user}")
        print(f"Assistant: {turn.assistant[:100]}...")
        print(f"Type: {turn.query_type}\n")

    client.close()

    print("="*80)
    print("âœ… Demo completed!")
    print("="*80)


if __name__ == "__main__":
    try:
        demo_enhanced()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Demo interrupted")
    except Exception as e:
        logger.error(f"Error: {e}")
        import traceback
        traceback.print_exc()
